{"name":"pfs","tagline":"A git-like distributed file system for a dockerized world.","body":"# Pachyderm File System\r\n\r\n## pfs v0.3 - Pachyderm MapReduce\r\nPfs v0.3 is the first pfs release to include support for MapReduce.\r\n##The MapReduce API\r\n\r\nWe’ve added a new pfs keyword `job`. Here’s how you use it:\r\n\r\n###Creating a new job\r\n\r\nJobs are specified as JSON files in the following format:\r\n\r\n```\r\n{\r\n    \"type\"  : either \"map\" or \"reduce\"\r\n    \"input\" : a file in pfs or the output from another job\r\n    \"image\" : the Docker image to use (which gets pulled from the Docker registry)\r\n    \"command\" : the command to start your web server\r\n}\r\n```\r\n\r\n**NOTE**: You do not need to specify the output location for a job. The output of a job, often referred to as a _materialized view_, is automatically stored in pfs `/job/<jobname>`.\r\n\r\n###POSTing a job to pfs\r\n\r\nPost a local JSON file with the above format to pfs:\r\n\r\n```sh\r\n$ curl -XPOST <host>/job/<jobname> -T <localfile>.json\r\n```\r\n\r\n**NOTE**: POSTing a job doesn't run the job. It just records the specification of the job in pfs. \r\n\r\n###Running a job\r\nJobs are only run on a commit. That way you always know exactly the state of\r\nthe file system that is used in a computation. To run all committed jobs, use\r\nthe `commit` keyword with the `run` parameter.\r\n\r\n```sh\r\n$ curl -XPOST <host>/commit?run\r\n```\r\n# Roadmap\r\nv0.3 will contain the first implementation of Dockerized MapReduce which will allow us to start doing actual distributed computations with pfs. You can track development [here](https://github.com/pachyderm-io/pfs/issues/4).\r\n\r\nThink of adding jobs as constructing a\r\n[DAG](http://en.wikipedia.org/wiki/Directed_acyclic_graph) of computations that\r\nyou want performed. When you call `/commit?run`, Pachyderm automatically\r\nschedules the jobs such that a job isn't run until the jobs it depends on have\r\ncompleted.\r\n\r\n###Getting the output of a job\r\nEach job records its output in its own read-only file system. You can read the output of the job with:\r\n\r\n```sh\r\n$ curl <host>/job/<jobname>/file/*?commit=<commit>\r\n```\r\n\r\n**NOTE**: You must specify the commit you want to read from and that commit\r\nneeds to have been created with the run parameter. We're planning to expand\r\nthis API to make it not have this requirement in the near future.\r\n\r\n## What is pfs?\r\nPfs is a distributed file system built specifically for the Docker\r\necosystem. You [deploy it with Docker](https://registry.hub.docker.com/u/pachyderm/pfs/),\r\njust like other applications in your stack. Furthermore,\r\nMapReduce jobs are specified as Docker containers, rather than .jars,\r\nletting you perform distributed computation using any tools you want.\r\n\r\n## Key Features\r\n- Fault-tolerant architecture built on [CoreOS](https://coreos.com) (implemented)\r\n- [Git-like distributed file system](#what-is-a-git-like-file-system) (implemented)\r\n- [Dockerized MapReduce](#what-is-dockerized-mapreduce) (not implemented)\r\n\r\n## Is pfs production ready\r\nNo, pfs is at Alpha status. [We'd love your help. :)](#how-do-i-hack-on-pfs)\r\n\r\n## Where is this project going?\r\nPachyderm will eventually be a complete replacement for Hadoop, built on top of\r\na modern toolchain instead of the JVM. Hadoop is a mature ecosystem, so there's\r\na long way to go before pfs will fully match its feature set. However, thanks to innovative tools like btrfs, Docker, and CoreOS, we can build an order of magnitude more functionality with much less code.\r\n\r\n## What is a \"git-like file system\"?\r\nPfs is implemented as a distributed layer on top of btrfs, the same\r\ncopy-on-write file system that powers Docker. Btrfs already offers\r\n[git-like semantics](http://zef.me/6023/who-needs-git-when-you-got-zfs/) on a\r\nsingle machine; pfs scales these out to an entire cluster. This allows features such as:\r\n- Commit-based history: File systems are generally single-state entities. Pfs,\r\non the other hand, provides a rich history of every previous state of your\r\ncluster. You can always revert to a prior commit in the event of a\r\ndisaster.\r\n- Branching: Thanks to btrfs's copy-on-write semantics, branching is ridiculously\r\ncheap in pfs. Each user can experiment freely in their own branch without\r\nimpacting anyone else or the underlying data. Branches can easily be merged back in the main cluster.\r\n- Cloning: Btrfs's send/receive functionality allows pfs to efficiently copy\r\nan entire cluster's worth of data while still maintaining its commit history.\r\n\r\n## What is \"dockerized MapReduce?\"\r\nThe basic interface for MapReduce is a `map` function and a `reduce` function.\r\nIn Hadoop this is exposed as a Java interface. In Pachyderm, MapReduce jobs are\r\nuser-submitted Docker containers with http servers inside them. Rather than\r\ncalling a `map` method on a class, Pachyderm POSTs files to the `/map` route on\r\na webserver. This completely democratizes MapReduce by decoupling it from a\r\nsingle platform, such as the JVM.\r\n\r\nThanks to Docker, Pachyderm can seamlessly integrate external libraries. For example, suppose you want to perform computer\r\nvision on a large set of images. Creating this job is as simple as\r\nrunning `npm install opencv` inside a Docker container and creating a node.js server, which uses this library on its `/map` route.\r\n\r\n## Quickstart Guide\r\nThe easiest way to try out pfs is to point curl at the live instance we have\r\nrunning here: http://146.148.35.127. We'll try to keep it up and running throughout\r\nthe day.\r\n\r\n### Creating a CoreOS cluster\r\nPfs is designed to run on CoreOS. To start, you'll need a working CoreOS\r\ncluster. Here's links on how to set one up:\r\n\r\n- [Google Compute Engine](https://coreos.com/docs/running-coreos/cloud-providers/google-compute-engine/) (recommended)\r\n- [Amazon EC2](https://coreos.com/docs/running-coreos/cloud-providers/ec2/)\r\n- [Vagrant](https://coreos.com/docs/running-coreos/platforms/vagrant/) (requires setting up DNS)\r\n\r\n### Deploy pfs\r\nSSH in to one of your new CoreOS machines.\r\n\r\n```shell\r\n$ wget https://github.com/pachyderm-io/pfs/raw/master/deploy/static/1Node.tar.gz\r\n$ tar -xvf 1Node.tar.gz\r\n$ fleetctl start 1Node/*\r\n```\r\n\r\nThe startup process takes a little while the first time you run it because\r\neach node has to pull a Docker image.\r\n\r\n### Checking the status of your deploy\r\nThe easiest way to see what's going on in your cluster is to use `list-units`\r\n```shell\r\n$ fleetctl list-units\r\n```\r\n\r\nIf things are working correctly, you should see something like:\r\n\r\n```\r\nUNIT                            MACHINE                         ACTIVE  SUB\r\nannounce-master-0-1.service     3817102d.../10.240.199.203      active  running\r\nmaster-0-1.service              3817102d.../10.240.199.203      active  running\r\nrouter.service                  3817102d.../10.240.199.203      active  running\r\n```\r\n\r\n### Using pfs\r\nPfs exposes a git-like interface to the file system:\r\n\r\n#### Creating files\r\n```shell\r\n# Write <file> to <branch>. Branch defaults to \"master\".\r\n$ curl -XPOST pfs/file/<file>?branch=<branch> -d @local_file\r\n```\r\n\r\n#### Reading files\r\n```shell\r\n# Read <file> from <master>.\r\n$ curl pfs/file/<file>\r\n\r\n# Read all files in a <directory>.\r\n$ curl pfs/file/<directory>/*\r\n\r\n# Read <file> from <commit>.\r\n$ curl pfs/file/<file>?commit=<commit>\r\n```\r\n\r\n#### Deleting files\r\n```shell\r\n# Delete <file> from <branch>. Branch defaults to \"master\".\r\n$ curl -XDELETE pfs/file/<file>?branch=<branch>\r\n```\r\n\r\n#### Committing changes\r\n```shell\r\n# Commit dirty changes to <branch>. Defaults to \"master\".\r\n$ curl -XPOST pfs/commit?branch=<branch>\r\n\r\n# Getting all commits.\r\n$ curl -XGET pfs/commit\r\n```\r\n\r\n#### Branching\r\n```shell\r\n# Create <branch> from <commit>.\r\n$ curl -XPOST pfs/branch?commit=<commit>&branch=<branch>\r\n\r\n# Commit to <branch>\r\n$ curl -XPOST pfs/commit?branch=<branch>\r\n\r\n# Getting all branches.\r\n$ curl -XGET pfs/branch\r\n```\r\n###MapReduce\r\n\r\n####Creating a job:\r\n\r\n```\r\n# Job format:\r\n{\r\n    \"type\"  : either \"map\" or \"reduce\"\r\n    \"input\" : a file in pfs or the output from another job\r\n    \"image\" : the Docker image\r\n    \"command\" : the command to start your web server\r\n}\r\n```\r\n\r\n```shell\r\n# Create or modify <job>:\r\n$ curl -XPOST <host>/job/<jobname> -T <localfile>.json\r\n```\r\n#### Deleting jobs\r\n\r\n```shell\r\n# Delete <job>\r\n$ curl -XDELETE <host>/job/<job>\r\n```\r\n\r\n#### Getting jobs\r\n\r\n```shell\r\n# Read <job>\r\n$ curl -XGET <host>/job/<job>\r\n```\r\n\r\n#### Running jobs\r\n\r\n```shell\r\n# Commit and run all jobs:\r\n$ curl -XPOST <host>/commit?run\r\n```\r\n\r\n#### Getting output from jobs\r\n\r\n```shell\r\n# Read <file> from the output of <job> at <commit>:\r\n$ curl -XGET <host>/job/<job>/file/<file>?commit=<commit>\r\n\r\n# Read the output of <job> at <commit>:\r\n$ curl -XGET <host>/job/<job>/file/*?commit=<commit>\r\n```\r\n\r\n## Who's building this?\r\nTwo guys who love data and communities and both happen to be named Joe. We'd love\r\nto chat: joey@pachyderm.io jdoliner@pachyderm.io.\r\n\r\n## How do I hack on pfs?\r\nPfs's only dependency is Docker. You can build it with:\r\n```shell\r\npfs$ docker build -t username/pfs .\r\n```\r\nDeploying what you build requires pushing the built container to the central\r\nDocker registry and changing the container name in the .service files from\r\n`pachyderm/pfs` to `username/pfs`.\r\n","google":"UA-56700403-1","note":"Don't delete this file! It's used internally to help with page regeneration."}